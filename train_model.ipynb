{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30b0afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs:\n",
    "    def __init__(self):\n",
    "        self.manifest_file = \"total_am.txt\"\n",
    "        self.labels_path = \"labels.csv\"\n",
    "        self.train_ratio = 0.8\n",
    "        self.num_workers = 4\n",
    "        self.batch_size = 64\n",
    "        self.sample_mode = 'random' #'smart'\n",
    "        self.teacher_forcing_ratio = 0.0\n",
    "        \n",
    "        self.num_classes = 2001\n",
    "        self.d_model = 512\n",
    "        self.d_ff = 2048\n",
    "        self.num_heads = 4\n",
    "        self.num_layers = 3\n",
    "        self.model_name = \"BERT\"\n",
    "        \n",
    "configs = Configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3067f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tokenizer import Tokenizer\n",
    "from data_module import DataModule\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(label_file=configs.labels_path)\n",
    "data_module = DataModule(configs, tokenizer)\n",
    "train_dataloader = data_module.get_dl(\"train\")\n",
    "valid_dataloader = data_module.get_dl(\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac4ce0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import Transformer_LM\n",
    "\n",
    "model = Transformer_LM(\n",
    "    num_classes=configs.num_classes,\n",
    "    d_model=configs.d_model,\n",
    "    d_ff=configs.d_ff,\n",
    "    num_heads=configs.num_heads,\n",
    "    num_layers=configs.num_layers,\n",
    "    model=configs.model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "582d2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbcf1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "Loss = CrossEntropyLoss(tokenizer)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08ee8a67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m logits, preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_lengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m Loss(logits, targets)\n\u001b[1;32m     13\u001b[0m perplexity \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(loss)\n",
      "File \u001b[0;32m~/anaconda3/envs/lm/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/바탕화면/language_model/Model.py:78\u001b[0m, in \u001b[0;36mTransformer_LM.forward\u001b[0;34m(self, inputs, input_lengths)\u001b[0m\n\u001b[1;32m     75\u001b[0m     input_var \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(step_output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     76\u001b[0m     logits\u001b[38;5;241m.\u001b[39mappend(input_var)\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_outputs, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "\n",
    "\n",
    "writer = SummaryWriter('runs/bert')\n",
    "\n",
    "for iteration, (inputs, seq_lengths, targets) in enumerate(train_dataloader):\n",
    "    inputs = inputs.cuda()\n",
    "    targets = targets.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    logits, preds = model(inputs, seq_lengths)\n",
    "    loss = Loss(logits, targets)\n",
    "    perplexity = torch.exp(loss)\n",
    "    writer.add_scalar(\"train_loss\", loss, iteration)\n",
    "    writer.add_scalar(\"train_perplexity\", perplexity, iteration)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if iteration % 1000 == 0 and iteration != 0:\n",
    "        VAL_LOSS = 0\n",
    "        val_iter = 0\n",
    "        for i, (val_inputs, val_lengths, val_targets) in enumerate(valid_dataloader):\n",
    "            if i > 100:\n",
    "                break\n",
    "            val_inputs = val_inputs.cuda()\n",
    "            val_targets = val_targets.cuda()\n",
    "            with torch.no_grad():\n",
    "                logits, preds = model(val_inputs, val_lengths)\n",
    "            val_loss = Loss(logits, val_targets)\n",
    "            VAL_LOSS += val_loss\n",
    "            val_iter += 1\n",
    "        validation_loss = VAL_LOSS/val_iter\n",
    "        validation_perplexity = torch.exp(validation_loss)\n",
    "        writer.add_scalar(\"validation_loss\", validation_loss, iteration)\n",
    "        writer.add_scalar(\"validation_perplexity\", validation_perplexity, iteration)\n",
    "            \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61bcacd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bert.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
