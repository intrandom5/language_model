{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30b0afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs:\n",
    "    def __init__(self):\n",
    "        self.manifest_file = \"total_am.txt\"\n",
    "        self.labels_path = \"aihub_labels.csv\"\n",
    "        self.train_ratio = 0.8\n",
    "        self.num_workers = 4\n",
    "        self.batch_size = 64\n",
    "        self.sample_mode = 'random' #'smart'\n",
    "        self.teacher_forcing_ratio = 0.0\n",
    "        \n",
    "        self.num_classes = 2001\n",
    "        self.d_model = 512\n",
    "        self.d_ff = 2048\n",
    "        self.num_heads = 4\n",
    "        self.num_layers = 3\n",
    "        self.model_name = \"BERT\"\n",
    "        \n",
    "configs = Configs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3067f5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tokenizer import Tokenizer\n",
    "from data_module import DataModule\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(label_file=configs.labels_path)\n",
    "data_module = DataModule(configs, tokenizer)\n",
    "train_dataloader = data_module.get_dl(\"train\")\n",
    "valid_dataloader = data_module.get_dl(\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac4ce0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import Transformer_LM\n",
    "\n",
    "model = Transformer_LM(\n",
    "    num_classes=configs.num_classes,\n",
    "    d_model=configs.d_model,\n",
    "    d_ff=configs.d_ff,\n",
    "    num_heads=configs.num_heads,\n",
    "    num_layers=configs.num_layers,\n",
    "    model=configs.model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "582d2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbcf1ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import CrossEntropyLoss, Perplexity\n",
    "from torch.optim import Adam\n",
    "\n",
    "Loss = CrossEntropyLoss(tokenizer)\n",
    "optimizer = Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee8a67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "\n",
    "\n",
    "writer = SummaryWriter('runs/bert')\n",
    "\n",
    "for iteration, (inputs, seq_lengths, targets) in enumerate(train_dataloader):\n",
    "    inputs = inputs.cuda()\n",
    "    targets = targets.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    logits, preds = model(inputs, seq_lengths)\n",
    "    loss = Loss(logits, targets)\n",
    "    perplexity = torch.exp(loss)\n",
    "    writer.add_scalar(\"train_loss\", loss, iteration)\n",
    "    writer.add_scalar(\"train_perplexity\", perplexity, iteration)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if iteration % 1000 == 0 and iteration != 0:\n",
    "        VAL_LOSS = 0\n",
    "        val_iter = 0\n",
    "        for i, (val_inputs, val_lengths, val_targets) in enumerate(valid_dataloader):\n",
    "            if i > 100:\n",
    "                break\n",
    "            val_inputs = val_inputs.cuda()\n",
    "            val_targets = val_targets.cuda()\n",
    "            with torch.no_grad():\n",
    "                logits, preds = model(val_inputs, val_lengths)\n",
    "            val_loss = Loss(logits, val_targets)\n",
    "            VAL_LOSS += val_loss\n",
    "            val_iter += 1\n",
    "        validation_loss = VAL_LOSS/val_iter\n",
    "        validation_perplexity = torch.exp(validation_loss)\n",
    "        writer.add_scalar(\"validation_loss\", validation_loss, iteration)\n",
    "        writer.add_scalar(\"validation_perplexity\", validation_perplexity, iteration)\n",
    "            \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61bcacd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3472c521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 좋은 날씨<mask><mask>난 사무실에 앉아서<mask>뭘<mask>하는거\n",
      " 좋은 날씨  난 사무실에 앉아서 뭘 하는거\n"
     ]
    }
   ],
   "source": [
    "# Test Bert Model\n",
    "\n",
    "sample = \"이 좋은 날씨에 난 사무실에 앉아서 뭘 하는거람\"\n",
    "\n",
    "tokenizer.idx2char[2000] = \"<mask>\"\n",
    "\n",
    "mask_indexes = random.sample(range(len(sample)), 3)\n",
    "\n",
    "#inputs = [tokenizer.sos_token] + tokenizer.encode(sample) + [tokenizer.eos_token]\n",
    "inputs = tokenizer.encode(sample)\n",
    "for idx in mask_indexes:\n",
    "    inputs[idx] = 2000\n",
    "    \n",
    "print(tokenizer.decode(inputs[1:-1]))\n",
    "\n",
    "input_length = [len(inputs)]\n",
    "\n",
    "inputs = torch.Tensor(inputs).unsqueeze(0).int()\n",
    "\n",
    "inputs = inputs.cuda()\n",
    "\n",
    "logits, preds = model(inputs, input_length)\n",
    "\n",
    "for idx in mask_indexes:\n",
    "    # <sos> 빼고\n",
    "    inputs[0][idx] = preds[0][idx+1]\n",
    "    \n",
    "result = tokenizer.decode(inputs[0][1:-1])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df5395af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model.load_state_dict(torch.load(\"gpt.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49d50113",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이건 근의 공식을 \n",
      "이건 근의 공식을  \n",
      "이건 근의 공식을  립\n",
      "이건 근의 공식을  립요\n",
      "이건 근의 공식을  립요어\n",
      "이건 근의 공식을  립요어지\n",
      "이건 근의 공식을  립요어지 \n",
      "이건 근의 공식을  립요어지 까\n",
      "이건 근의 공식을  립요어지 까 \n",
      "이건 근의 공식을  립요어지 까  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이건 근의 공식을  립요어지 까  '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_generator(sentence: str, length: int):\n",
    "    inputs = tokenizer.encode(sentence)\n",
    "    inputs = torch.Tensor(inputs).unsqueeze(0).long().cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(length):\n",
    "            # 문장 뒤에 아무 글자나 붙여서 다음 글자까지 예측하도록 함.\n",
    "            blank = torch.Tensor([[5]]).cuda()\n",
    "            inputs = torch.cat((inputs, blank), dim=1).long()\n",
    "            input_lengths = torch.Tensor(inputs.size(1)).long().cuda()\n",
    "            logits, preds = model.forward(inputs, input_lengths)\n",
    "            \n",
    "            # 앞의 글자들은 바꿀 것이 아니기 때문에 예측한 맨 마지막 글자만 뒤에 추가한다!\n",
    "            inputs[0][-1] = preds[0][-1]\n",
    "#             inputs = torch.cat((inputs, preds[-1].unsqueeze(0)), dim=1)\n",
    "            print(tokenizer.decode(inputs))\n",
    "    return tokenizer.decode(inputs)\n",
    "\n",
    "text_generator(\"이건 근의 공식을\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a4ef693",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"gpt.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
