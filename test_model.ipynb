{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21dc6f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tokenizer import Tokenizer\n",
    "from data_module import DataModule\n",
    "from Model import Transformer_LM\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class Configs:\n",
    "    def __init__(self):\n",
    "        self.manifest_file = \"total_am.txt\"\n",
    "        self.labels_path = \"aihub_labels.csv\"\n",
    "        self.train_ratio = 0.8\n",
    "        self.num_workers = 4\n",
    "        self.batch_size = 64\n",
    "        self.sample_mode = 'random' #'smart'\n",
    "        self.teacher_forcing_ratio = 0.0\n",
    "        \n",
    "        self.num_classes = 2001\n",
    "        self.d_model = 512\n",
    "        self.d_ff = 2048\n",
    "        self.num_heads = 4\n",
    "        self.num_layers = 3\n",
    "        self.model_name = \"BERT\"\n",
    "        \n",
    "bert_configs = Configs()\n",
    "\n",
    "tokenizer = Tokenizer(label_file=bert_configs.labels_path)\n",
    "tokenizer.idx2char[2000] = \"<mask>\"\n",
    "\n",
    "model = Transformer_LM(\n",
    "    num_classes=bert_configs.num_classes,\n",
    "    d_model=bert_configs.d_model,\n",
    "    d_ff=bert_configs.d_ff,\n",
    "    num_heads=bert_configs.num_heads,\n",
    "    num_layers=bert_configs.num_layers,\n",
    "    model=bert_configs.model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98a1d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test BERT Model\n",
    "model.load_state_dict(torch.load(\"bert.pt\"))\n",
    "\n",
    "def text_corrector(sentence: str, mask_num: int):\n",
    "    # sentence : 입력 문장\n",
    "    # mask_num : masking 할 문자 수\n",
    "    mask_indexes = random.sample(range(len(sentence)), mask_num)\n",
    "\n",
    "    inputs = [tokenizer.sos_token] + tokenizer.encode(sentence) + [tokenizer.eos_token]\n",
    "    for idx in mask_indexes:\n",
    "        #inputs[idx] = 2000\n",
    "        inputs[idx] = random.randrange(5, 2000)\n",
    "    print(\"input :\", tokenizer.decode(inputs[1:-1]))\n",
    "\n",
    "    input_length = [len(inputs)]\n",
    "    \n",
    "    inputs = torch.Tensor(inputs).unsqueeze(0).int()\n",
    "\n",
    "    logits, preds = model(inputs, input_length)\n",
    "\n",
    "    for idx in mask_indexes:\n",
    "        inputs[0][idx] = preds[0][idx]\n",
    "\n",
    "    return tokenizer.decode(inputs[0][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e69f118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : 이뿍게 좋은 날씨에 사낡짓홍서 뿍하는 거람\n",
      "output : 이렇게 좋은 날씨에 사람 해서  하는 거람\n"
     ]
    }
   ],
   "source": [
    "print(\"output :\", text_corrector(\"이렇게 좋은 날씨에 사무실에서 뭐하는 거람\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c305a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_configs = Configs()\n",
    "\n",
    "gpt_configs.num_classes = 2000\n",
    "gpt_configs.model_name = \"GPT\"\n",
    "\n",
    "gpt_model = Transformer_LM(\n",
    "    num_classes=gpt_configs.num_classes,\n",
    "    d_model=gpt_configs.d_model,\n",
    "    d_ff=gpt_configs.d_ff,\n",
    "    num_heads=gpt_configs.num_heads,\n",
    "    num_layers=gpt_configs.num_layers,\n",
    "    model=gpt_configs.model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73db7c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test GPT gpt_model\n",
    "gpt_model.load_state_dict(torch.load(\"models/gpt.pt\"))\n",
    "\n",
    "def text_generator(sentence: str, length: int):\n",
    "    # sentence : 입력 문장\n",
    "    # length : 몇 글자를 더 이어 쓸것인지\n",
    "    inputs = tokenizer.encode(sentence)\n",
    "    inputs = torch.Tensor(inputs).unsqueeze(0).long()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(length):\n",
    "            # 문장 뒤에 아무 글자나 붙여서 다음 글자까지 예측하도록 함.\n",
    "            blank = torch.Tensor([[5]])\n",
    "            inputs = torch.cat((inputs, blank), dim=1).long()\n",
    "            input_lengths = torch.Tensor(inputs.size(1)).long()\n",
    "            logits, preds = gpt_model.forward(inputs, input_lengths)\n",
    "            \n",
    "            # 앞의 글자들은 바꿀 것이 아니기 때문에 예측한 맨 마지막 글자만 뒤에 추가한다!\n",
    "            inputs[0][-1] = preds[0][-1]\n",
    "            print(tokenizer.decode(inputs))\n",
    "    return tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae58713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이렇게 좋을 \n",
      "이렇게 좋을 그\n",
      "이렇게 좋을 그요\n",
      "이렇게 좋을 그요 \n",
      "이렇게 좋을 그요 요\n",
      "이렇게 좋을 그요 요그\n",
      "이렇게 좋을 그요 요그 \n",
      "이렇게 좋을 그요 요그  \n",
      "이렇게 좋을 그요 요그   \n",
      "이렇게 좋을 그요 요그   요\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'이렇게 좋을 그요 요그   요'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\"이렇게 좋을\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de58e42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
